{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neuronales Netz in Python 3.x erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook wird ein vollständiges neuronales Netz erstellt. Dabei wird Schritt für Schritt beginnend mit dem Gerüstcode bishin zu einem Feed Forward Netz ein funktionsfähiges Netz aufgebaut, welches anhand verschiedener Eingangsdaten trainiert,getestet und als Klassifikator verwendet werden kann.\n",
    "\n",
    "Das Netz und diese Unterlagen basieren auf dem Buch: Rashid, Neuronale Netze selbst programmieren, O`Reilly-Verlag, 2018 \n",
    "\n",
    "Also lassen wir es klein beginnen. Los geht´s!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Gerüstcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wird eine Klasse \"Neuronales Netz\" (neuralNetwork) skizziert und im Anschlußß werden drei Grundfunktionen hinzugefügt.\n",
    "1. Initialisierung: Anzahl der Knoten für Eingabeschicht, verdeckte Schicht und Ausgabeschicht werden festgelegt\n",
    "2. Trainieren: Gewichte werden anhand von Trainingsbeispielen verfeinert. D.h. das Netz wird angelernt\n",
    "3. Abfragen: Von den Ausgabeknoten wird ein passender Vorhersagewert abgefragt, basierend auf der jeweiligen Eingangsbelegung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Initialisierung des Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst definieren wir die Anzahl der benötigten Eingabe-, verdeckte Schicht- und Ausgangsknoten. Dies wird mittels Parameter übermittelt für das spätere Objekt neuralNetwork. So können beliebig viele Netze unterschiedlicher Größe erstellt werden. \n",
    "\n",
    "Dies ist eine der Grundregeln guter Programmierung. Möglichst keinen spezifischen Code sonder einen allgemein gehaltenen Code zu erstellen. So können in kürzester Zeit verschiedene Lösungen auf demselben Urtyp (Klasse) entwickelt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Zeile 3 wurden die notwendigen Parameter in der def __init__() angelegt. Diese werden wie in der Zeile 4 zu sehen explizit für eine Instanz (also ein Objekt der Klasse neuralNetwork()) übergeben.\n",
    "1. self.inodes = Die Anzahl der Eingangsknoten. Wir legen in Zeile 4 drei Eingangsknoten fest. Es könnten jedoch auch beliebig mehr viele mehr sein.\n",
    "2. self.hnodes = Die Anzahl der Knoten der verdeckten Schicht.\n",
    "3. self.onodes = Die Anzahl der Knoten des Ausgabeschicht. \n",
    "Es wurde bei allen Parametern in Zeile 4 jeweils drei Knoten angelegt. Zusätzlich wird eine Lernrate angelegt. Dazu später mehr!\n",
    "Und ein Objekt \"n\" der Klasse neuralNetwork() wurde erstellt und die entsprechenden Daten übermittelt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Anlegen eines Objekts der Klasse neuralNetwork() können wir nun innerhalb dieser speziellen Instanz Daten übergeben. Der aktuelle Arbeitsstand in Kapitel 2.2 ist zum bis hier die Zeilen 3 und 4 welche in der gezeigten Reihenfolge erstellt werden müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verknüpfungsgewichte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie aus dem Theorieteil noch bekannt, sind die Gewichte ausschlaggebend für die Signalanteile der Vorwärtsrichtung des Netzes. Zusätzlich relevant werden die Gewichte, wenn der Fehleranteil in Rückwärtsrichtung das Netz durchläuft und die Gewichte korrigiert.\n",
    "\n",
    "Die Gewichte können als Matrix ausgedrückt werden und werden wie folgt erzeugt:\n",
    "1. Matrix für die Gewichte zwischen der Eingabeschicht und der versteckten Schicht (W_input_hidden = hidden_nodes mal input_nodes)\n",
    "2. Matrix für die Gewichte zwischen der versteckten Schicht und der Ausgabeschicht (W_hidden_output = output_nodes mal hidden_nodes)\n",
    "\n",
    "Die Anfangswerte eines nicht trainierten Netzes sind klein und zufällig. Dies kann mittels einer numpy-Funktion für Zufallszahlen für einen Array generiert werden. Das Array ist dabei rows mal columns groß (siehe Zeile 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-352a9f220efa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "numpy.random.rand(rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy ist noch nicht definiert. Das heisst wir müssen die Bibliothek zunächst importieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43965178, 0.48788907, 0.3106531 ],\n",
       "       [0.55384594, 0.29073304, 0.57014052],\n",
       "       [0.99706599, 0.47426319, 0.70297327]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.rand(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da der Bereich später Werte zwischen -1 und 1 annimmt, müssen wir dies auch in der Initialisierung beachten. In unserem Fall machen wir es uns einfach und schreiben einfach ein -0.5 hinter die numpy-Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46041091,  0.29215525, -0.29984478],\n",
       "       [-0.08356642, -0.23655556, -0.01928136],\n",
       "       [-0.15171192,  0.11599062,  0.2871416 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.rand(3,3)-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes übertragen wir dies auf die beiden Matrizen der jeweiligen Verknüpfungsgewichte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-209c54b84f76>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-209c54b84f76>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    next layer\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# link weight matrices, wih and who\n",
    "# weights inside the arrays are w_i_j, where link is from node i to node j in the \n",
    "next layer\n",
    "# w11 w21\n",
    "# w12 w22 etc \n",
    "self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5)\n",
    "self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++Die Fehlermeldung ignorieren. Sie kommt nur auf da der Quellcode aus dem Kontext gerissen wurde.++ \n",
    "\n",
    "Zur Erläuterung ist es so allerdings einfacher.\n",
    "Was sehen wir hier:\n",
    "Die Größe der Matrizen ergibt sich aus den Werten in self.inodes, self.hnodes und self.onodes.\n",
    "\n",
    "Der bis hier erstellte Code ist in Zeile 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n",
    "\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5)\n",
    "        self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5)\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Das Netz abfragen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion query() übergibt die Ausgabewerte aus der Ausgabeschicht. Dazu sind folgende Punkte wichtig:\n",
    "1. Weiterleitung der Eingabesignale von den Knoten der Eingabeschicht über die versteckte Schicht zur Ausgabeschicht.\n",
    "2. Übernahme der Verknüpfungsgewichte, welche die Signale moderieren zwischen den jeweiligen Schichten.\n",
    "3. Die Eingangssignale eines Knotens werden in dieser Funktion über die Sigmoid-Aktivierungsfunktion in die Ausgangssignale gewandelt.\n",
    "\n",
    "Diese umfangreiche Aufgaben erledigt Python dank der numpy-Bibliothek in wenigen Schritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a18ea4d424af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhidden_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_inputs = numpy.dot(self.wih, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anweisung in Zeile 14 wendet die FUnktion für das Punktprodukt von Matrizen aus der numpy-Bibliothek auf die Verknüpfungsgewichte W_input_hidden und die Eingänge I an. Es werden dadurch alle Eingänge mit den korrekten Verknüpfungsgewichten der Knoten der versteckten Schicht zusammengefasst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Sigmoid-Aktivierungsfunktion findet sich unter dem etwas eigenwilligen Namen expit() in der Python-Bibliothek scipy. Da wir die Aktivierungsfunktion nur einmal im Objekt definieren müssen und mehrfach experimentell verwenden, packen wir dies auch in die Initialisierungsfunktion. Der folgende Code definiert die Aktivierungsfunktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b6d95565ce0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# activation function is the sigmoid function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# activation function is the sigmoid function\n",
    "self.activation_function = lambda x: scipy.special.expit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++Die Fehlermeldung ignorieren. Sie kommt nur auf da der Quellcode aus dem Kontext gerissen wurde.++ \n",
    "\n",
    "In dieser Anweisung wird eine Funktion erzeugt. lambda übernimmt hier in aller Kürze die def()-Definition. lambda vereinfacht dies, so dass alles in einer Zeile geschrieben werden kann. lamba-Funktion sind namenslos oder anonym.\n",
    "\n",
    "Die Funktion übernimmt den Wert x und gibt scipy.special.expit(x) also das Ergebnis der auf x angewendeten Sigmoidfunktion. Übergeben wird die ganze Arbeit dann an die Funktion self.activation_function(). Sobald dies aufgerufen wird, wird die Sigmoidfunktion auf den Übergabewert angewandt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1c216e3fb0e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# calculate the signals emerging from hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhidden_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# calculate the signals emerging from hidden layer\n",
    "hidden_outputs = self.activation_function(hidden_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++Die Fehlermeldung ignorieren. Sie kommt nur auf da der Quellcode aus dem Kontext gerissen wurde.++\n",
    "\n",
    "In Zeile 17 wird nun die eigentliche Aufgabe gemacht. D.h. auf die zusammengefassten Signale eines Knotens wird die Sigmoidfunktion angewandt. Die Ausgangssignale der Knoten der versteckten Schicht werden dadurch in die Matrix hidden_outputs überführt.\n",
    "\n",
    "Somit wäre die versteckte Schicht fertig. Die Ausgabeschicht folgt diesem Muster und sieht dem oberen Code sehr ähnlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-674a0ced18fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# calculate signals into hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhidden_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# calculate the signals emerging from hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhidden_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# calculate signals into hidden layer\n",
    "hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "# calculate the signals emerging from hidden layer\n",
    "hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "# calculate signals into final output layer\n",
    "final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "# calculate the signals emerging from final output layer\n",
    "final_outputs = self.activation_function(final_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++Die Fehlermeldung ignorieren. Sie kommt nur auf da der Quellcode aus dem Kontext gerissen wurde.++\n",
    "\n",
    "Wir kommen somit zu bislang folgendem Stand des Codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy math functions (arrays and more)\n",
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5)\n",
    "        self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5)\n",
    "        #self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes,self.inodes))\n",
    "        #self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes,self.hnodes))\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion query() benötigt zur Arbeit nur die input_list. Alles weitere übernehmen dann die entsprechend genannten Funktionen. \n",
    "\n",
    "Somit sind wir mit den Funktionen init und query fertig. Zuguterletzt folgt noch die train()-Funktion in Kapitel 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Zwischenfazit - Testlauf des Netzes ohne Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu testen, ob unser bisher erstelltes Netz funktioniert, erzeugen wir ein jeweils 3-Knoten je Schicht neuronales Netz mit zufälliger Eingangsbelegung von (1.0, 0.5,–1.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### 1. Import der benötigten Bibs ###################################\n",
    "\n",
    "# numpy math functions (arrays and more)\n",
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "\n",
    "##################### 2. Klasse des Neuronalen Netzes #################################\n",
    "\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5)\n",
    "        self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5)\n",
    "        #self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes,self.inodes))\n",
    "        #self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes,self.hnodes))\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "\n",
    "##################### 3. Testobjekt der oberen Klasse #################################    \n",
    "    \n",
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45499852],\n",
       "       [0.52485148],\n",
       "       [0.46194549]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################### 4. Testabfrage für einen Eingangswert von 1.0 0.5 und -1.5 ##############################\n",
    "##################### Ziel ist es einen Ausgangswert zu bekommen ##############################################\n",
    "n.query([1.0,0.5,-1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was wird hier nun alles umgesetzt:\n",
    "1. Import der benötigten Bibliotheken um z.B. die Matrizenrechnung oder die Sigmoid-Aktivierungsfunktion übernehmen zu können.\n",
    "2. Erstellen einer neuronalen Netz-Klasse. Hierin sind nun zwei der drei Funktionen fertig. \n",
    "    a. def __init__(): initialisieren der Knoten in den drei Schichten, Erstinitialiserung der Gewichtematrix mit Zufallswerten, initialisieren der Lernrate welche wir gleich im Training benötigen und die Berechnung der Aktivierungsfunktion für die Ausgabewerte je Knoten.\n",
    "    b. def train(): Diese Funktion kommt in Abschnitt 2.6 dran.\n",
    "    c. def query(): Übertragung der Eingangsliste in die jeweiligen Knotenmatrizen, Gewichtete Signale berechnen aufgrund der Eingangssignale, Ausgabe der Ausgabeliste der Ausgangsknoten.\n",
    "3. Erstellen eines Objekts in Abhängigkeit der Klasse aus 2. In diesem Fall mit je drei Knoten und einer Lernrate von 0,3. Dieses Objekt heißt schlicht nur n\n",
    "4. Testwerte eintragen, welche mögliche Ausgabewerte liefern. Wäre das Netz bereits trainiert, würden wir hier passende Werte zu unseren Eingabewerte bekommen. Bsp. das Netz würde die vermutlich korrekte Klasse vorhersagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Training des Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Funktion kommen zwei Trainingsphasen zum Tragen.\n",
    "1. Ermittlung einer Ausgabe auf Basis der Trainingswerte. Ähnlich der query()-Funktion.\n",
    "2. Die berechnete Ausgabe aus 1. wird mit der gewünschten Ausgabe verglichen und steuert die Gewichtaktualsierung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "def train(self, inputs_list, targets_list):\n",
    "    # convert inputs list to 2d array\n",
    "    inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "    targets = numpy.array(targets_list, ndmin=2).T\n",
    "    # calculate signals into hidden layer\n",
    "    hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "    # calculate the signals emerging from hidden layer\n",
    "    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "    # calculate signals into final output layer\n",
    "    final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "    # calculate the signals emerging from final output layer\n",
    "    final_outputs = self.activation_function(final_inputs)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der erste Teil des Codes ähnelt der Funktion query(). Einzig der zusätzliche Parameter targets_list was für die gewünschte Zielantwort steht. Diese werden ähnlich wie die inputs_list in ein numpy-Array übertragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-85c778c5f7fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# error is the (target - actual)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'targets' is not defined"
     ]
    }
   ],
   "source": [
    "# error is the (target - actual)\n",
    "output_errors = targets - final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++Die Fehlermeldung ignorieren. Sie kommt nur auf da der Quellcode aus dem Kontext gerissen wurde.++\n",
    "\n",
    "In Zeile 19 ermitteln wir nun die Fehler. Genauer die Differenz zwischen der gewünschten Ausgabe und der Ausgabe, welche das Trainigsbeispiel geliefert hat. (targets - final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-93349d92c536>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# hidden layer error is the output_errors, split by weights, recombined at hidden nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhidden_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwho\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "hidden_errors = numpy.dot(self.who.T, output_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++Die Fehlermeldung ignorieren. Sie kommt nur auf da der Quellcode aus dem Kontext gerissen wurde.++\n",
    "\n",
    "In Zeile 20 wird nun der zurückgeführte Fehler entsprechend der Verbidungsgewichte proportional je Knoten und Schicht aufgeteilt (Backpropagation). Dies wird mit der Matrixform \"errorshidden  = weights(Transponiert)hidden_output  · errorsoutput\" in der Theorie erarbeitet. Mittels numpy kann in Python das Punktprodukt elegant umgesetzt werden. Output Errors steht hierbei für die Gewichte zwischen der versteckten Schicht und der Ausgabeschicht und hidden Errors steht für die Gewichte zwischen der Eingabeschicht und der versteckten Schicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier folgt nochmals die entsprechende Formel zur Gewichtskorrektur für die Verknüpfung zwischen zwei Knoten in aufeinanderfolgenden Schichten. Der Faktor alpha ist die Lernrate, und die Funktion sigmoid() ist die schon bekannte\n",
    "Sigmoid-Aktivierungsfunktion. Die Punkte stehen für die Punktprodukte der Matrizen. Die letzte Komponenten ist die transponierte Matrix der Ausgänge der vorherigen Schicht. Praktisch heisst das, dass die Ausgangsspalte zu einer Ausgangszeile wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"files/gewichtskorrektur.png\" \"Test\" width=500px></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-099319a31da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# update the weights for the links between the hidden and output layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwho\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_errors\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfinal_outputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# update the weights for the links between the hidden and output layers\n",
    "self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++Die Fehlermeldung ignorieren. Sie kommt nur auf da der Quellcode aus dem Kontext gerissen wurde.++\n",
    "\n",
    "Hier nun erstmal der Code für die Gewichte zwischen der versteckten Schicht und der Ausgabeschicht. Die Lernrate self.lr wird mit dem Rest des Ausdrucks multipliziert worauf anschliessend mit numpy.dot() eine Matrizenmultiplikation ausgeführt wird.\n",
    "+= bedeutet, dass der ermittelte Wert zu self.who hinzuaddiert wird.\n",
    "\n",
    "Was entspricht nun was aus der Formel:\n",
    "alpha = self.lr (Lernrate)\n",
    "Ek = output_errors (ermittelter Fehler der nächsten Schicht)\n",
    "Ok(1-Ok) = final_outputs*(1.0-final_outputs) (Sigmoidkomponenten der nächsten Schicht)\n",
    "OjT = numpy.transpose(hidden_outputs) (transponierte Ausgabe der vorherigen Schicht)\n",
    "\n",
    "Für die Eingabeschicht zur verdeckten Schicht sieht der Code ähnlich aus. Siehe in der Folgezeile beide Aktualisierungen der Gewichte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-17b8cc97c2ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# update the weights for the links between the hidden and output layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwho\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_errors\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfinal_outputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# update the weights for the links between the input and hidden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwih\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_errors\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhidden_outputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhidden_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# update the weights for the links between the hidden and output layers\n",
    "self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "# update the weights for the links between the input and hidden layers\n",
    "self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das war´s! Nun schauen wir uns mal den vollständigen Code an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Vollständiges Neuronales Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zeile steht nun das vollständige Feed Forward Neural Net als Klasse samt der benötigten Bibliotheken. und das ist gar nicht soviel Code in Anbetracht der Tatsache, was damit nun alles gemacht werden kann.\n",
    "\n",
    "Wie sehen die einzelnen Schritte nochmals aus:\n",
    "\n",
    "1. Import der benötigten Bibs\n",
    "1.1 numpy Bibliothek für mathematische Funktionen (Matrizen)\n",
    "1.2 scipy Bibliothek für die Sigmoid-Funktion expit()\n",
    "\n",
    "2. Definition der Klasse unseres neuronalen Netzes\n",
    "\n",
    "2.1 initialisieren des neuronalen Netzes __init__()\n",
    "2.1.1 Festlegen der Anzahl Knoten in den jeweiligen Schichten. Diese Werte werden als Parameter übergeben.\n",
    "2.1.2 Knoten verbinden und Gewichte belegen\n",
    "2.1.3 Lernrate festlegen (alpha. Bsp. Bergabstieg beim Gradientenverfahren. Zu große oder zu kleine Sprünge könnten das Minimum übergehen)\n",
    "2.1.4 Sigmoid-Funktion als Aktivierungsfunktion festlegen und berechnen lassen\n",
    "\n",
    "2.2 Trainingseinheit des Neuronalen Netzes train()\n",
    "2.2.1 convert inputs list to 2d array\n",
    "2.2.2 calculate signals into hidden layer\n",
    "2.2.3 calculate the signals emerging from hidden layer\n",
    "2.2.4 calculate signals into final output layer\n",
    "2.2.5 calculate the signals emerging from final output layer\n",
    "2.2.6 output layer error is the (target - actual)\n",
    "2.2.7 hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "2.2.8 update the weights for the links between the hidden and output layers\n",
    "2.2.9 update the weights for the links between the input and hidden layers\n",
    "\n",
    "2.3 query the neural network\n",
    "2.3.1 convert inputs list to 2d array\n",
    "2.3.2 calculate signals into hidden layer\n",
    "2.3.3 calculate the signals emerging from hidden layer\n",
    "2.3.4 calculate signals into final output layer\n",
    "2.3.5 calculate the signals emerging from final output layer\n",
    "2.3.6 Feedback of the neural net\n",
    "3. Testobjekt der oberen Klasse\n",
    "3.1 number of input, hidden and output nodes\n",
    "3.2 learning rate is 0.3\n",
    "3.3 create instance of neural network\n",
    "4. Testabfrage für einen Eingangswert von ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### 1. Import der benötigten Bibs ###################################\n",
    "\n",
    "# 1.1 numpy math functions (arrays and more)\n",
    "import numpy\n",
    "# 1.2 scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "\n",
    "##################### 2. Klasse des Neuronalen Netzes #################################\n",
    "\n",
    "# 2. neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # 2.1 initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # 2.1.1 set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # 2.1.2 link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5)\n",
    "        self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5)\n",
    "        #self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes,self.inodes))\n",
    "        #self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes,self.hnodes))\n",
    "        \n",
    "        # 2.1.3 learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # 2.1.4 activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # 2.2 train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # 2.2.1 convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # 2.2.2 calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # 2.2.3 calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # 2.2.4 calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # 2.2.5 calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # 2.2.6 output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # 2.2.7 hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # 2.2.8 update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # 2.2.9 update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    # 2.3 query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # 2.3.1 convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # 2.3.2 calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # 2.3.3 calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # 2.3.4 calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # 2.3.5 calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # 2.3.6 Feedback of the neural net\n",
    "        return final_outputs\n",
    "\n",
    "##################### 3. Testobjekt der oberen Klasse #################################    \n",
    "    \n",
    "# 3.1 number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# 3.2 learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "# 3.3 create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40382958],\n",
       "       [0.50612328],\n",
       "       [0.39185957]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################### 4. Testabfrage für einen Eingangswert von 1.0 0.5 und -1.5 ##############################\n",
    "##################### Ziel ist es einen Ausgangswert zu bekommen ##############################################\n",
    "n.query([1.0,0.5,-1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Code kann nun vollständig übernommen werden und auf verschieden Übungen angewandt werden. In Kapitel 3 wird nun ein Beispieldatensatz eingefügt, traininert, getestet und seine Performance (grob die Treffergenauigkeit) aufgezeigt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
