{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neuronale Netze trainieren, testen und prüfen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Kapitel 2 wurde nun ein vollständig lauffähiges Neuronales Netz erstellt. Wir können bei diesem Netz die Knotenanzahl, die Lernrate anpassen. Um ein solches Netz mit Trainings- und Testdaten zu befüllen, müssen noch ein paar weitere Punkte beachtet werden. Diese folgen in diesem Kapitel. \n",
    "\n",
    "\n",
    "Das Kapitel ist dabei in folgende Schritte aufgeteilt:\n",
    "1. Daten einlesen\n",
    "2. Feature-Daten (Eingabewerte) auf ein Niveau zwischen 0 und 1 skalieren und in einen Fließkommazahl-Array überführen\n",
    "3. Label-Daten (Ausgabewerte) auf ein Niveau zwischen 0 und 1 skalieren und in einen Integer-Array überführen\n",
    "4. Die Skalierung mittels einer For-Schleife für alle Eingabe- und Ausgabewerte durchführen\n",
    "5. Epochen einführen. D.h. das Netz darf mehrmals das Training mit denselben Daten durchlaufen um sich zu optimieren\n",
    "6. prozentuale Performancemessung durchführen. Basierend auf dem Label wird geprüft wie gut die Trefferquote war\n",
    "7. Netz vollständig trainieren, testen und prüfen\n",
    "8. Optimierungsmöglichkeiten für das Netz aufzeigen (Lernrate anpassen, Epochen anpassen, Netzstruktur ändern, Mehr Daten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Daten einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Daten einlesen zu können, müssen wir zunächst diese Datei aufbereitet haben. Wichtig hierbei sind folgende Punkte:\n",
    "\n",
    "1. Keine Überschriften\n",
    "2. Label in der ersten Spalte\n",
    "3. Label ist eine Ziffer\n",
    "4. Keine Kommatas für 1,5 € sondern 1.5. Wir nutzen die amerikanische Schreibweise. Die Trenner zwischen den Spalten wird hingegen mit Kommatas gemacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"data/iris_dataset/iris_one.csv\", 'r')\n",
    "data_list = data_file.readlines()\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lassen wir uns zunächst die Länge der Tabelle anzeigen. Also die Anzahl Zeilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Anzeige der Länge der Liste\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und nun geben wir den Inhalt der ersten Zeile (also die 0 eintragen) direkt hier aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,7.9,3.8,6.4,2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inhalt der Liste ausgeben\n",
    "data_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt mal eine andere Liste einladen. Z.B. schwertlilie_test oder schwertlilie_train oder mnist_dataset/mnist_train_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Featuredaten skalieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Featuredaten aus Zeile 3 sind in einem Niveau zwischen 0 und 5.7. Das ist nicht so geschickt für unser Netz, da wir Zahlen zwischen 0 und 1 erwarten. Daher müssen wir diese Zahlen auf dieses Niveau bringen. Damit wir jedoch keine 0 und auch keine 1 herausbekommen wenden wir einen Trick an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,7.9,3.8,6.4,2']\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "data_file = open(\"data/iris_dataset/iris_one.csv\", 'r')\n",
    "data_list = data_file.readlines()\n",
    "data_file.close()\n",
    "\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir sehen ist die erste Zahl eine 1. Das ist das Label für diesen Datensatz. Die zweite Zahl ist eine 5,7 und zugleich die größte Zahl. Wenn wir den Ursprungsdatensatz anschauen und in Excel mit der MAX()-Funktion prüfen, so werden wir erkennen, dass die größte Zahl eine 7,9 ist. Dies ist also unser Ausgangsskalierungsfaktor für die Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '7.9', '3.8', '6.4', '2']\n",
      "[1.         0.48620253 0.81202532 0.26063291]\n"
     ]
    }
   ],
   "source": [
    "all_values = data_list[0].split(',')\n",
    "\n",
    "scaled_input = (numpy.asfarray(all_values[1:]) / 7.9 * 0.99) + 0.01\n",
    "\n",
    "print(all_values)\n",
    "print(scaled_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Zeile 5 haben wir folgendes getan:\n",
    "\n",
    "1. Der Datensatz 0 wurde ausgewählt und dieser String nach den Kommatas mit der Funktion split() aufgetrennt. Das Ergebnis wird in all_values abgelegt.\n",
    "2. In der nächsten Zeile passieren einige Dinge gleichzeitig. all_values[1:] bedeutet, dass alle Elemente ab der Position 1 betroffen sind. Also alle Elemente außer dem Label auf Position 0. \n",
    "3. Die Funktion numpy.asfarray() konvertiert den bisherigen Textstring in echte Zahlen. \n",
    "4. Diese Zahlen teilen wir durch die maximal mögliche Zahl (7,9).\n",
    "5. Um die Werte aus 1. auf 0,0 bis 0,99 zu bekommen müssen diese Werte mit 0,99 multipliziert werden.\n",
    "6. Anschließend werden die Werte mit 0,01 addiert, sodass wir keine reinen 0-Werte bekommen.\n",
    "\n",
    "Der direkte Vergleich der Zahlen (noch als Textwert) und die skalierten Werte werden per Print ausgegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Labelwerte skalieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weitestgehend analog gehen wir nun mit den Labeldaten vor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0.01 0.99 0.01]\n"
     ]
    }
   ],
   "source": [
    "#output nodes is 3 (example)\n",
    "onodes = 3\n",
    "targets = numpy.zeros(onodes) + 0.01\n",
    "targets[int(all_values[0])] = 0.99\n",
    "\n",
    "print(all_values[0])\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was passiert nun in Zeile 6:\n",
    "\n",
    "1. Die Anzahl an Ausgabeknoten wird auf 3 gesetzt. Das liegt daran, dass wir im IRIS-Datensatz nur drei verschiedene Blütensorten haben.\n",
    "2. numpy.zeros() erzeugt ein mit Nullen gefülltes Array. Größe und Gestalt des Arrays mit der Länge \"onodes\" wird als Parameter übergeben. Hinzuaddiert wird 0,01 um reine Nullwerte zu vermeiden.\n",
    "3. Hier passiert nun wieder etwas mehr:\n",
    "4. all_values[0] ist das Label/die Kennung des Datensatzes. In unserem Fall trägt das Label eine 1. Es ist also die zweite Blütensorte mit der Kennung und dem Array-Index 1 (möglich wären: 0, 1, 2).\n",
    "5. Diese Kennung wird ein eine Ganzzahl gewandelt und \n",
    "6. mit dem array-index verbunden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Skalierung und Konvertierung auf alle Labels und Featuredaten anwenden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im obigen Beispiel hatten wir nur jeweils einen Datensatz eingelesen. Wir sollten dies jedoch sowohl für den Trainingsdatensatz wie auch unseren Testdatensatz anwenden. Dies können wir mit einer For-Schleife elegant lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "output_nodes = 3\n",
    "\n",
    "training_data_file = open(\"data/iris_dataset/iris_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in training_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 7.9*0.99) + 0.01\n",
    "    # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "    targets = numpy.zeros(output_nodes) + 0.01\n",
    "    # all_values[0] is the target label for this record\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    # n.train ist noch auskommentiert, da dies später auf unser Netz-Objekt n angewandt werden soll.\n",
    "    #n.train(inputs, targets)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben den bekannten Elementen aus den vorherigen Schritten ist nun ein weiterer Punkt hinzugekommen. Die For-Schleife durchläuft die training_data_list solange bis kein Datensatz mehr auszulesen ist. Jeder Datensatz wird dabei einzeln mit obigen Schritten bearbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4.3,3,1.1,0.1\n",
      "\n",
      "[0.54886076 0.38594937 0.1478481  0.02253165]\n"
     ]
    }
   ],
   "source": [
    "print(record)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99 0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Gleiche machen wir dann noch mit den Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "test_data_file = open(\"data/iris_dataset/iris_test.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 7.9 * 0.99) + 0.01    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0.68670886 0.38594937 0.57392405 0.19797468]\n"
     ]
    }
   ],
   "source": [
    "print(correct_label)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlichkeiten zum Trainingsdatensatz sind vorhanden. Neu ist, dass wir das Label (also den korrekten Wert) in die Variable correct_label schreiben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warum werden immer nur ein Wert ausgegeben? Das liegt daran, dass wir mit der For-Schleife den kompletten Datensatz mit 130 Trainingsdatensatz und 20 Testdatensätze zwar durchlaufen. Jedoch werden diese nicht wie zuvor in Listen abgelegt. Wir berechnen hier lediglich die entsprechenden skalierten Werte um diese an das Netz in der jeweiligen Funktion des Trainings oder Tests zu übergeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Epochen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochen sind Trainingswiederholungen. Mit weiteren Anzahl Epochen wird derselbe Trainingsdatensatz erneut durchlaufen. Dies dient dazu den Gradientenabstieg zu verfeinern. Siehe auch folgende Abbildung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"files/gradientenverfahren_alpha_sprung.png\" \"Test\" width=600px></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochen benötigen wir demnach für den Trainingsdatensatz. Dies ist mittels einer weiteren For-Schleife schnell getan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# epochs is the number of times the training data set is used for training\n",
    "epochs = 3\n",
    "\n",
    "output_nodes = 3\n",
    "\n",
    "training_data_file = open(\"data/iris_dataset/iris_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# train the neural network\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 7.9*0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        # n.train ist noch auskommentiert, da dies später auf unser Netz-Objekt n angewandt werden soll.\n",
    "        #n.train(inputs, targets)\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Performancemessung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Qualität und die Aussagekraft unseres Netzes messen zu können, werden die durch das nun gewichtete Netz ermittelten Aussagewerte mit den tatsächlichen Labels verglichen. In unserem Beispiel-Datensatz haben wir wie oben erwänht 20 Testdatensätze. Sollten 10 korrekt sein und 10 falsch, so werden wir eine Performance von 0,5 oder 50% bekommen. Um dies zu ermitteln gehen wir in den Testdatensatz und schreiben jedes Mal mit was das Netz gemutmaßt hat und was tatsächlich korrekt war. Dies schreiben wir in eine temporäre Liste und am Ende wird verglichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d495d5a17fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masfarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m7.9\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# query the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m# the index of the highest value corresponds to the label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "test_data_file = open(\"data/iris_dataset/iris_test.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "\n",
    "\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 7.9*0.99) + 0.01\n",
    "    # query the network\n",
    "    outputs = n.query(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = numpy.argmax(outputs)\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Quellcode funktioniert leider nur, mit dem Netz. Was wir im Anschluß uns dann gleich mal ansehen werden.\n",
    "\n",
    "Doch zunächst gehen wir auf die neuen Elemente ein.\n",
    "1. scorecard[] --> Scorecard ist eine leere Trefferliste, welche nach jedem Datensatz aktualisiert wird.\n",
    "2. Das Netz wählt den größten Wert der Ausgabeknoten als Antwort. Der Index dieses Knotens wird mit der Kennung, also dem Wert im Label verglichen numpy.argmax()\n",
    "3. Mittels if-else wird geprüft, ob das Label korrekt vorhergesagt wurde oder nicht. Entsprechend wird der Trefferliste eine 1 (Treffer korrekt) oder eine 0 (Treffer falsch) eingetragen.\n",
    "\n",
    "Das Beste wäre wir überführen unsere Erkenntnisse nun vollständig in das Netz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 vollständiges Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 [0]\n",
      "0 2 [0, 0]\n",
      "0 2 [0, 0, 0]\n",
      "0 1 [0, 0, 0, 0]\n",
      "0 0 [0, 0, 0, 0, 1]\n",
      "0 0 [0, 0, 0, 0, 1, 1]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "0 1 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0 0 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "0 0 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "0 2 [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "performance =  0.2\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "##################### 1. Import der benötigten Bibs ###################################\n",
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "# library for plotting arrays\n",
    "#import matplotlib.pyplot\n",
    "# ensure the plots are inside this notebook, not an external window\n",
    "#%matplotlib inline\n",
    "\n",
    "#######################################################################################\n",
    "##################### 2. Anpassung der Variablen ######################################\n",
    "# number of input, hidden and output nodes\n",
    "input_nodes = 4\n",
    "hidden_nodes = 10\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.3\n",
    "\n",
    "# epochs is the number of times the training data set is used for training\n",
    "epochs = 3\n",
    "\n",
    "# load the training data CSV file into a list\n",
    "training_data_file = open(\"data/iris_dataset/iris_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# load the test data CSV file into a list\n",
    "test_data_file = open(\"data/iris_dataset/iris_test.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "\n",
    "#######################################################################################\n",
    "##################### 3. Klasse des Neuronalen Netzes #################################\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "\n",
    "#######################################################################################\n",
    "##################### 4. Erstellen eines Objekts der obigen Klasse ####################\n",
    "    \n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n",
    "\n",
    "#######################################################################################\n",
    "##################### 5. Das Netz basierend auf den Epochen trainieren ################\n",
    " \n",
    "\n",
    "# train the neural network\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 7.9*0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets)\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "#######################################################################################\n",
    "##################### 6. Das Netz auf Basis der Testdaten prüfen ######################\n",
    "\n",
    "# test the neural network\n",
    "\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 7.9*0.99) + 0.01\n",
    "    # query the network\n",
    "    outputs = n.query(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = numpy.argmax(outputs)\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    print(label,correct_label,scorecard)\n",
    "    pass\n",
    "\n",
    "#######################################################################################\n",
    "##################### 7. Ausgabe der Genauigkeit des Netzes (Performance) #############\n",
    "\n",
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)\n",
    "\n",
    "#######################################################################################\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit einer Performance von 20% ist das Netz noch optimierungsfähig. Trotzdem wollen wir mal sehen wie gut ein spezieller Datensatz vorhergesagt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Ersten Datensatz auslesen\n",
    "all_values=test_data_list[0].split(',')\n",
    "# Ausgabe des Labels\n",
    "print(all_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76319629],\n",
       "       [0.14158856],\n",
       "       [0.16869803]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.query((numpy.asfarray(all_values[1:]) / 7.9 * 0.99) + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Netz liest zunächst das Label aus. Eine 2. In der anschliessenden Vorhersage liegt die 2 bei 17%. Das Netz würde hier eine 0 mit 75% vorhersagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Optimierungsmöglichkeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lernrate**: Unter Umständen muß die Lernrate zu einem späteren Zeitpunkt angepasst werden. Diese Lernrate wird notwendig um den Sprung im Gradientenverfahren nicht zu klein oder zu groß werden zu lassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"files/gradientenverfahren_alpha_sprung.png\" \"Test\" width=600px></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainingswiederholungen**: Mit weiteren Anzahl Epochen wird derselbe Trainingsdatensatz erneut durchlaufen. Dies dient dazu den Gradientenabstieg zu verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mehr Daten**: Je größer die Datenmengen für das Training, desto genauer werden die Ergebnisse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Netzstruktur ändern**: Anzahl der Knoten der versteckten Schicht anpassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Netz optimieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 [1]\n",
      "2 2 [1, 1]\n",
      "2 2 [1, 1, 1]\n",
      "1 1 [1, 1, 1, 1]\n",
      "0 0 [1, 1, 1, 1, 1]\n",
      "0 0 [1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1 1 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0 0 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0 0 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "performance =  1.0\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "##################### 1. Import der benötigten Bibs ###################################\n",
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "# library for plotting arrays\n",
    "#import matplotlib.pyplot\n",
    "# ensure the plots are inside this notebook, not an external window\n",
    "#%matplotlib inline\n",
    "\n",
    "#######################################################################################\n",
    "##################### 2. Anpassung der Variablen ######################################\n",
    "# number of input, hidden and output nodes\n",
    "input_nodes = 4\n",
    "hidden_nodes = 10\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.3\n",
    "\n",
    "# epochs is the number of times the training data set is used for training\n",
    "epochs = 100\n",
    "\n",
    "# load the training data CSV file into a list\n",
    "training_data_file = open(\"data/iris_dataset/iris_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# load the test data CSV file into a list\n",
    "test_data_file = open(\"data/iris_dataset/iris_test.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "\n",
    "#######################################################################################\n",
    "##################### 3. Klasse des Neuronalen Netzes #################################\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "\n",
    "#######################################################################################\n",
    "##################### 4. Erstellen eines Objekts der obigen Klasse ####################\n",
    "    \n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n",
    "\n",
    "#######################################################################################\n",
    "##################### 5. Das Netz basierend auf den Epochen trainieren ################\n",
    " \n",
    "\n",
    "# train the neural network\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 7.9*0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets)\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "#######################################################################################\n",
    "##################### 6. Das Netz auf Basis der Testdaten prüfen ######################\n",
    "\n",
    "# test the neural network\n",
    "\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 7.9*0.99) + 0.01\n",
    "    # query the network\n",
    "    outputs = n.query(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = numpy.argmax(outputs)\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    print(label,correct_label,scorecard)\n",
    "    pass\n",
    "\n",
    "#######################################################################################\n",
    "##################### 7. Ausgabe der Genauigkeit des Netzes (Performance) #############\n",
    "\n",
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)\n",
    "\n",
    "#######################################################################################\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03119014],\n",
       "       [0.01436555],\n",
       "       [0.93431681]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ersten Datensatz auslesen\n",
    "all_values=test_data_list[0].split(',')\n",
    "# Ausgabe des Labels\n",
    "print(all_values[0])\n",
    "n.query((numpy.asfarray(all_values[1:]) / 7.9 * 0.99) + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Maximalwert der Sets auslesen und automatisch eintragen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Maximalwert mußte bislang manuell in den Daten-Dateien ausgelesen werden. Dies kann mittels der numpy.amax()-Funktion automatisiert werden, sodass dieser Wert nicht hardcoded in den Quelltext eingetragen werden muß. Siehe folgenden Code bei 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 [1]\n",
      "2 2 [1, 1]\n",
      "2 2 [1, 1, 1]\n",
      "1 1 [1, 1, 1, 1]\n",
      "0 0 [1, 1, 1, 1, 1]\n",
      "0 0 [1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1 1 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0 0 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0 0 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "performance =  1.0\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "##################### 1. Import der benötigten Bibs ###################################\n",
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "# library for plotting arrays\n",
    "#import matplotlib.pyplot\n",
    "# ensure the plots are inside this notebook, not an external window\n",
    "#%matplotlib inline\n",
    "\n",
    "#######################################################################################\n",
    "##################### 2. Anpassung der Variablen ######################################\n",
    "# number of input, hidden and output nodes\n",
    "input_nodes = 4\n",
    "hidden_nodes = 10\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.3\n",
    "\n",
    "# epochs is the number of times the training data set is used for training\n",
    "epochs = 100\n",
    "\n",
    "# load the training data CSV file into a list\n",
    "training_data_file = open(\"data/iris_dataset/iris_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# load the test data CSV file into a list\n",
    "test_data_file = open(\"data/iris_dataset/iris_test.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "\n",
    "#######################################################################################\n",
    "##################### 2.1 Maximalfeaturewert auslesen #################################\n",
    "# max_test_train_set um alle Werte in eine Liste zu schreiben\n",
    "max_test_train_set = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = numpy.asfarray(all_values[1:])\n",
    "    max_test_train_set.append(inputs)  \n",
    "    pass\n",
    "\n",
    "for record in training_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = numpy.asfarray(all_values[1:])\n",
    "    max_test_train_set.append(inputs)  \n",
    "    pass\n",
    "\n",
    "#print(max_test_train_set)\n",
    "\n",
    "max_test_train_value=numpy.amax(max_test_train_set)\n",
    "\n",
    "#print(max_test_value)\n",
    "\n",
    "#######################################################################################\n",
    "##################### 3. Klasse des Neuronalen Netzes #################################\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "\n",
    "#######################################################################################\n",
    "##################### 4. Erstellen eines Objekts der obigen Klasse ####################\n",
    "    \n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n",
    "\n",
    "#######################################################################################\n",
    "##################### 5. Das Netz basierend auf den Epochen trainieren ################\n",
    " \n",
    "\n",
    "# train the neural network\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / max_test_train_value*0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets)\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "#######################################################################################\n",
    "##################### 6. Das Netz auf Basis der Testdaten prüfen ######################\n",
    "\n",
    "# test the neural network\n",
    "\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / max_test_train_value*0.99) + 0.01\n",
    "    # query the network\n",
    "    outputs = n.query(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = numpy.argmax(outputs)\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    print(label,correct_label,scorecard)\n",
    "    pass\n",
    "\n",
    "#######################################################################################\n",
    "##################### 7. Ausgabe der Genauigkeit des Netzes (Performance) #############\n",
    "\n",
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)\n",
    "\n",
    "#######################################################################################\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00640738],\n",
       "       [0.00735859],\n",
       "       [0.99657759]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################### Testabfrage für neue eigene Eingangswerte ###########################################\n",
    "##################### Mutmaßt das Netz die richtige Antwort? ##############################################\n",
    "# wähle dazu einen passenden Datensatz und trage die entsprechenden Featurewerte hier ein (bsp. Label=2 und Feature=5,2,3.5,1)\n",
    "\n",
    "n.query([5,2,3.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
